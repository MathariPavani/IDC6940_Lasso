---
title: "Feature Selection for House Price Prediction "
subtitle: "LASSO Regression for Data Science Project - spring 2025 "
author: "Mathari Pavani and Shalini Jonnadula "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction

Well, LASSO regression is one of the most used statistical regression techniques in feature selection and regularization. In simple terms, LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some of the coefficients all the way to zero. That encourages sparsity in the model, hence giving only the most relevant predictors. The aim of LASSO regression is to enhance model interpretability and improve predictive accuracy by selecting a subset of features that substantially contribute to variation in the outcome variable. To attain this, LASSO is based on optimization techniques and finds its application mostly in high-dimensional data, wherein other regression methods often fail due to overfitting issues because of a large number of features included in the analysis [@tibshirani1996regression].

It is relevant for various fields: in finance [@zou2006adaptive], in healthcare, or real estate, where usually huge datasets exist, including thousands of predictors that bear or may bear no relevance. In such applications, for instance, prediction of house prices, credit risks, and analyses of customer behaviors are ideal with the selection of important variables through LASSO from a wide array of variable banks. It also compares LASSO with other regularizations like Ridge regression and Elastic Net, which also aim to enhance the performance and reduce overfitting of a model but use different kinds of penalization [@hastie2017extended].

In this paper, we delve into the theoretical foundation of LASSO regression and its practical applications. We further compare the performance of other feature selection methods, including Ridge regression, and show how LASSO is more capable of handling high-dimensional data for tasks such as house price prediction. By understanding how LASSO works and how it impacts model accuracy, we seek to emphasize its utility as a tool for modern data analysis.




## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
