---
title: "Feature Selection for House Price Prediction "
subtitle: "LASSO Regression for Data Science Project - spring 2025 "
author: "Mathari Pavani and Shalini Jonnadula "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Data Set: [AMES Housing Dataset](https://www.kaggle.com/datasets/prevek18/ames-housing-dataset)
<br>
Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction

LASSO regression (Least Absolute Shrinkage and Selection Operator) is a well-known statistical regression technique for feature selection and regularization. LASSO adds an L1 penalty to regression coefficients, which pushes some coefficients towards zero, thus inducing sparsity in the model and retaining the most significant predictors. This property renders LASSO particularly useful in improving model interpretability and predictability, as it selects a subset of predictors that have an important impact on variation in the outcome variable. The principal benefit of LASSO is that it is capable of handling high-dimensional data, where other regression methods will struggle with overfitting due to the inclusion of numerous features[@tibshirani1996regression].

In reality, house price forecasting is an important but challenging activity. With so many variables influencing the price of a house—location, size, amenities, and condition of the economy, to mention but a few—the sheer volume of potential predictors makes it hard to build a useful predictive model. One of the main challenges of house price forecasting is selecting the most informative features from a vast set of variables.

LASSO is a variation of linear regression, which has been augmented with the addition of a penalty term that sets some of the regression coefficients to zero, effectively removing irrelevant or redundant features. This is an extremely helpful operation in high-dimensional data, where traditional linear regression models overfit because there are so many predictor variables. In contrast, LASSO enhances the model's generalization ability and therefore provides a more precise and stable estimate of house prices.

In this project, we will be working on different implementations of LASSO regression with a special focus on L1 regularization and its effect on feature selection. We will apply this technique to a house prices dataset, selecting the most predictive features and validating the accuracy of the model in predicting house prices based on the selected features.

Furthermore, we will also make a comparison between the efficiency of LASSO and other regularizing methods such as Ridge regression and Elastic Net, which are also attempting to prevent overfitting by incorporating different penalizing methods [@hastie2017extended]. From the comparison, we will outline the advantages and disadvantages of the use of LASSO to handle high-dimensional data and its applicability for house price estimation.

The ultimate goal of this project is to demonstrate the success of LASSO regression application for feature selection for predictive models targeting dimensionality reduction, model interpretability, and improved predictive ability. The application of this technique extends beyond real estate and hence has applications in a wide range of other fields such as finance, medicine, and more where large datasets with possibly redundant predictors exist. For example, in finance, LASSO is used to predict credit risks, and in medicine it is used to make patient risk predictions.

The objective of this project is to provide valuable information regarding the way LASSO regression works, how it differs from other forms of regularization, and how it should be used effectively in practical applications, for example, predicting house prices.


### Literature Review 

LASSO regression is one of the most important contributors to model sparsity and predictive accuracy, particularly when handling high-dimensional data. In Huan Xu, Constantine Caramanis, and Shie Mannor's work, the authors describe the connection between robust regression and LASSO, showing that under certain uncertainty conditions, the robust regression problem is equivalent to LASSO. This connection is important as it shows how LASSO can offer robustness to outliers while preserving the sparsity property, which is very important for feature selection. This sparsity and robustness concept is highly applicable to our project, where LASSO will be used to choose the most significant features to use in predicting house prices. The idea that LASSO can handle noisy data and still select important predictors resonates with our goal of building a stable and accurate model of house prices.[@xu2012robust]

In the paper by Carter Hill, the author explains how both LASSO and Ridge regression serve as alternatives to the traditional least squares regression, especially in the context of high-dimensional datasets. Hill discusses how both methods help mitigate overfitting, a common issue when working with a large number of predictors, and select the most important features for prediction. While Ridge regression penalizes the size of the coefficients, LASSO tends to shrink some coefficients to zero, effectively performing feature selection. Hill emphasizes the advantages of these methods in improving model interpretability and predictive accuracy. However, both methods have limitations in distinguishing correlation from causation, which is an important consideration in practical applications. This paper is particularly valuable for our project as it provides a foundation for understanding how LASSO helps in selecting the most predictive features for house price estimation by addressing overfitting and ensuring that only relevant variables are included.[@hill2016explaining]

The paper by Trevor Hastie, Robert Tibshirani, and Martin Wainwright provides a comprehensive review of LASSO in high-dimensional regression, covering its theoretical properties, including sparsity and oracle inequalities. It also explores practical aspects like coordinate descent algorithms and extensions such as Elastic Net, Group LASSO, and Adaptive LASSO. These extensions address challenges like multicollinearity and grouped variable selection. This paper’s insights on the theory and applications of LASSO, along with its comparisons to other regularization techniques, will help guide the application of LASSO in our house price prediction project by offering valuable methods for dealing with high-dimensional datasets.[@hastie2015lasso]

The paper by Ji Hyung Lee, Zhentao Shi, and Zhan Gao explores the use of LASSO in predictive regression when some regressors are heterogeneous time series. They discuss the limitations of Adaptive LASSO in such cases, where it cannot completely eliminate inactive cointegrating variables. To address this, the authors introduce Twin Adaptive LASSO (TAlasso), a two-step selection method. Their method improves consistency and enhances the prediction accuracy for datasets involving mixed-root regressors. This paper will help in understanding how LASSO-based methods can be extended for better accuracy in predictive modeling for house price prediction when handling complex relationships in the data.[@lee2020lasso]

Together, these papers will guide our approach to using LASSO for feature selection, ensuring robustness, accuracy, and interpretability in our house price prediction model. Each study presents a facet of LASSO's potential that can be leveraged to optimize our model and handle the complexity of high-dimensional housing data effectively.

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
